{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "Автор материала: Павел Нестеров (@mephistopheies). Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/100c3Ek94UL-VRwXrN4lxCSnGjfJrl6Gc96G21DNCh4w).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Anaconda3\\\\python.exe'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.3\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.13.3\n",
      "scipy 0.19.1\n",
      "pandas 0.20.3\n",
      "matplotlib 2.1.0\n",
      "sklearn 0.19.1\n",
      "\n",
      "compiler   : MSC v.1900 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   :\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = 'stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = 'top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jquery', 'python', 'javascript', 'php', 'c#', 'java', 'c++', 'ios', 'html', 'android'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font>В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=tags_top\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    # z = ...\n",
    "                    z = self._b[tag] \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        # z += ...\n",
    "                        z += self._w[tag][self._vocab[word]] \n",
    "                        \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    # sigma = ...\n",
    "                    if z >= 0:\n",
    "                        sigma = 1/(1 + np.exp(-z))\n",
    "                    else:\n",
    "                        sigma = 1 - 1/(1 + np.exp(z))\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    # sample_loss += ...\n",
    "                    if y == 1: \n",
    "                        sample_loss += -y*np.log(np.max([tolerance, sigma])) \n",
    "                    else: \n",
    "                        sample_loss += -(1 - y)*np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        # dLdw = ...\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3feb32244df410bbdda064dfed2ffdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5kAAAKnCAYAAAAMUYlPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21ce0333b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 20.04\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    def __init__(self, tags=top_tags):      \n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     accuracy_level=0.9):\n",
    "\n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy = []\n",
    "        with open(fname, 'r') as f:            \n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                sentence = sentence.split(' ')\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                sample_loss = 0\n",
    "                predicted_tags = None\n",
    "                \n",
    "                for tag in self._tags:\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    z = self._b[tag] \n",
    "                    for word in sentence:\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]] \n",
    "                        \n",
    "                    if z >= 0:\n",
    "                        sigma = 1/(1 + np.exp(-z))\n",
    "                    else:\n",
    "                        sigma = 1 - 1/(1 + np.exp(z))\n",
    "                    \n",
    "                    \n",
    "                    if y == 1: \n",
    "                        sample_loss += -y*np.log(np.max([tolerance, sigma])) \n",
    "                    else: \n",
    "                        sample_loss += -(1 - y)*np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                    \n",
    "                    if n < top_n_train:\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    else:\n",
    "                        if predicted_tags is None:\n",
    "                            predicted_tags = []\n",
    "                        if sigma > accuracy_level:\n",
    "                            predicted_tags.append(tag)\n",
    "                    \n",
    "                n += 1\n",
    "                                        \n",
    "                self._loss.append(sample_loss)\n",
    "                if predicted_tags is not None:\n",
    "                    accuracy.append(len(tags.intersection(predicted_tags))/len(tags.union(predicted_tags)))\n",
    "            \n",
    "        return(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71d132a56cc44c3b2ed430efa98e405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# \n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
